{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урок 2. Keras#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Автор: Шенк Евгений Станиславович"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>Попробуйте обучить, нейронную сеть на Keras(рассмотренную на уроке) на датасете MNIST с другими параметрами. \n",
    "        Опишите в комментарии к уроку - какой результата вы добились от нейросети? Что помогло вам улучшить ее точность?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изначальные параметры:\n",
    "\n",
    "Количество нейронов между внутренними слоями = 64,  \n",
    "Активация первых двух слоев = 'relu',  \n",
    "Активация последнего слоя = 'softmax' (подходит для данной ситуации лучше всего и ее не менял)  \n",
    "Оптимизатор = 'adam',  \n",
    "loss = 'categorical_crossentropy'  (не изменял)\n",
    "epochs = 5.  \n",
    "batch_size = 32.  \n",
    "\n",
    "При таких данных результат 0.9655\n",
    "\n",
    "params = [[64, 'relu',  64, 'relu', 10, 'softmax'], ['adam', 'categorical_crossentropy'], [5, 32]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменения оптимизатора и метода активации первых двух слоев сильного прироста не давали, хотя некоторые виды неподходили вовсе. При изменении только 'relu' на 'elu' результат accuracy: 0.9730, а при изменениии только 'adam' на 'Adadelta' результат accuracy: 0.9717.  \n",
    "\n",
    "Увеличение количества нейронов между внутренними слоями с 64 до 128 улучшело результат до accuracy: 0.9725\n",
    "Уменьшение - ухудшало результат: 32 - accuracy: ~0.95; 16 - accuracy: ~0.93\n",
    "\n",
    "Увеличение количества эпох улучшает результат, но увеличивает затраты времени на обучение и наоборот.\n",
    "\n",
    "Увеличение размера батча ускоряет процесс обучения, но ухудшает результат. Попытка поставить batch_size = 1 привела к увеличению затрат времени в 50 раз и ухудшению результата accuracy: ~0.92. После нескольких экспериментов изначальное значение batch_size = 32 оказалось оптимальным.\n",
    "\n",
    "Лучший результат при приемлемом времени получился (accuracy: 0.9951):  \n",
    "Количество нейронов между внутренними слоями = 128,  \n",
    "Активация первых двух слоев = 'elu',  \n",
    "Активация последнего слоя = 'softmax'  \n",
    "Оптимизатор = 'Adadelta',  \n",
    "loss = 'categorical_crossentropy'  (не изменял)  \n",
    "epochs = 15.  \n",
    "batch_size = 32.  \n",
    "\n",
    "params = [[128, 'elu',  128, 'elu', 10, 'softmax'], ['Adadelta', 'categorical_crossentropy'], [15, 32]]\n",
    "\n",
    "изменение batch_size 32 -> 128 accuracy: 0.9951 -> 0.9923, но отработало раза в 3 быстрее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import mnist\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2177)\n",
    "tf.random.set_seed(2177)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Flatten the images.\n",
    "train_images = train_images.reshape((-1, 784))\n",
    "test_images = test_images.reshape((-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [[128, 'elu',  128, 'elu', 10, 'softmax'], ['Adadelta', 'categorical_crossentropy'], [15, 32]] # accuracy: 0.9951\n",
    "\n",
    "# 'relu', 'sigmoid', 'softmax', 'softplus', 'softsign', 'tanh', 'selu', 'elu', 'exponential'\n",
    "# 'SGD', 'RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl' \n",
    "\n",
    "# softplus(x) = log(exp(x) + 1) # accuracy: 0.9710\n",
    "# tanh # accuracy: 0.9701\n",
    "# selu # accuracy: 0.9701\n",
    "# elu # accuracy: 0.9730\n",
    "\n",
    "# Adadelta #accuracy: 0.9717\n",
    "\n",
    "# epoch=7 # accuracy: 0.9721\n",
    "# epoch=15 # accuracy: 0.9852\n",
    "\n",
    "# params = [[64, 'relu',  64, 'relu', 10, 'softmax'], ['adam', 'categorical_crossentropy'], [5, 32]] # accuracy: 0.9655\n",
    "\n",
    "# params = [[64, 'elu',  64, 'elu', 10, 'softmax'], [opt, 'categorical_crossentropy'], [5, 32]]\n",
    "\n",
    "# params = [[64, 'relu', 64, 'relu', 10, 'softmax'], ['adam', 'categorical_crossentropy'], [5, 32]] # accuracy: 0.9733"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nopt = tf.keras.optimizers.Ftrl(\\n    learning_rate=0.001,\\n    learning_rate_power=-0.5,\\n    initial_accumulator_value=0.1,\\n    l1_regularization_strength=0.0,\\n    l2_regularization_strength=0.0,\\n    name=\"Ftrl\",\\n    l2_shrinkage_regularization_strength=0.0\\n)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "opt = tf.keras.optimizers.Ftrl(\n",
    "    learning_rate=0.001,\n",
    "    learning_rate_power=-0.5,\n",
    "    initial_accumulator_value=0.1,\n",
    "    l1_regularization_strength=0.0,\n",
    "    l2_regularization_strength=0.0,\n",
    "    name=\"Ftrl\",\n",
    "    l2_shrinkage_regularization_strength=0.0\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3128 - accuracy: 0.9023\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1365 - accuracy: 0.9580\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0981 - accuracy: 0.9693\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0771 - accuracy: 0.9764\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0636 - accuracy: 0.9799\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0531 - accuracy: 0.9832\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0459 - accuracy: 0.9854\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0400 - accuracy: 0.9870\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0335 - accuracy: 0.9889\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0288 - accuracy: 0.9904\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0268 - accuracy: 0.9913\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0219 - accuracy: 0.9928\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0202 - accuracy: 0.9932\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0180 - accuracy: 0.9938\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0146 - accuracy: 0.9951\n",
      "10000/10000 [==============================] - 0s 13us/step\n",
      "[7 2 1 0 4]\n",
      "[7 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Dense(params[0][0], activation=params[0][1], input_shape=(784,)),\n",
    "  Dense(params[0][2], activation=params[0][3]),\n",
    "  Dense(params[0][4], activation=params[0][5]),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  optimizer=params[1][0],\n",
    "  loss=params[1][1],\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=params[2][0],\n",
    "  batch_size=params[2][1]\n",
    ")\n",
    "\n",
    "# Evaluate the model.\n",
    "model.evaluate(\n",
    "  test_images,\n",
    "  to_categorical(test_labels)\n",
    ")\n",
    "\n",
    "# Save the model to disk.\n",
    "# model.save_weights('model.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('model.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7\n",
      " 1 2 1 1 7 4 2 3 5 1 2 4 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7\n",
      " 1 2 1 1 7 4 2 3 5 1 2 4 4]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:50])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:50]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
