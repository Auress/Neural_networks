{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урок 4. Сверточные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Автор: Шенк Евгений Станиславович"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое задание\n",
    "\n",
    "<ol>\n",
    "    <li>Попробовать улучшить точность распознования образов cifar 10 сверточной нейронной сетью, рассмотренной на уроке. Приложить анализ с описанием того, что улучшает работу нейронной сети и что ухудшает.\n",
    "    </li>\n",
    "    <li>Описать также в анализе какие необоходимо внести изменения  в получившуюся у вас нейронную сеть если бы ей нужно было работать не с cifar10, а с MNIST, CIFAR100 и IMAGENET.\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение параметров нейросети приводило:  \n",
    "1. Увеличение размера батча приводит к ускорению работы сети, т.к. увеличивается количество подаваемых данных за раз. До некоторого момента заметного падения качества модели не происходит, далее увеличение нецелесообразно.\n",
    "2. Увеличение количества эпох приводит к улучшению качества модели, до того, как модель начнет переобучиваться и качество на тестовых данных перестанет расти. Пропорционально увеличивает время на обучение модели.\n",
    "3. Аугментация данных увеличивает количество входных данных и используется в случае их нехватки. В данной сети прироста качества НЕ давала, т.к. в cifar10 данных, судя по всему, хватает.\n",
    "4. Dropout позволяет сети не переобучаться за счет исключения некого процента нейронов. В данном случае исключение данного слоя каких-то серьезных изменений НЕ вызвало.\n",
    "5. Слой Pulling уплотняет признаки, проводя их преобразование (здесь выбирается максимум из 2 на 2). Исключение данного слоя приводит к существенному росту затрат времени на обучение, но и качество немного выросло. В случае использования схемы 3 на 3 в обоих слоях качество модели упало, т.к. потери информации при уплотнении увеличились.\n",
    "6. Изменение kernel_size в слоях с 3 на 3 на 5 на 5 никак на качестве модели не отразилось.\n",
    "7. Увеличение скорости обучения в оптимизаторе в 10 раз, привело к улучшению качества модели, судя по чему скорость была слишком низкой и модель недообучалась. Дальнейшее увеличение скорости резко ухудшило качество, т.к., судя по всему, модель начала проскакивать минимум.\n",
    "\n",
    "В случае если бы нужно было работать не с cifar10, а с MNIST, CIFAR100 и IMAGENET, то пришлось бы изменять:  \n",
    "1. Количество получаемых классов до 100 в CIFAR100 и огромного количества в IMAGENET (судя по wiki там 14,197,122 изображений по 21,841 категориям).\n",
    "2. Изменение количества слоев и сложности сети в целом, т.к. для MNIST избыточна, а для других недостаточна.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
