{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урок 5. Рекуррентные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Автор: Шенк Евгений Станиславович"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое задание\n",
    "\n",
    "<ol>\n",
    "    <li>Попробуйте изменить параметры нейронной сети работающей с датасетом imdb так, чтобы улучшить ее точность. Приложите анализ.</li>\n",
    "    <li>Попробуйте изменить параметры нейронной сети генерирующий текст таким образом, чтобы добиться генерации как можно более осмысленного текста. Пришлите лучший получившейся у вас текст и опишите, что вы предприняли, чтобы его получить. Можно использовать текст другого прозведения.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1.  \n",
    "Изменение параметров нейросети приводило:  \n",
    "    1. Изменение размера батча с 50 до 250 привело к серьезному ускорению работы сети и отсутствию заметных потерь качества, т.к. мы увеличили порции подачи данных и данных у нас достаточно и это не привело к падению качества.\n",
    "    2. Изменение количества эпох приводило только к увеличению затрат времени на обучение, для данной сети хватает и одного прохода.\n",
    "    3. Изменение слоя с LSTM на GRU или SimpleRNN приводило к ускорению обучения, но качество падано, связано с принципами работы данных алгоритмов.\n",
    "    4. Изменение параметра maxlen с 80 до 180 немного улучшило качество модели, но увеличило время обучения, 80 слов было недостаточно.\n",
    "    5. Изменение параметра dropout=0.2, recurrent_dropout=0.2 до 0.05 тоже немного (но почти незаметно) улучшило качество модели, т.к. количество выбрасываемой информации уменьшилось, а переобучение не наступает.\n",
    "    6. Изменение оптимайзеров и их параметров приводило только к ухудшению качества.\n",
    "В общем единственное что заметно положительно влияет на работу, это увеличение размера батча (ускоряет процесс), все остальное можно оставить как есть.\n",
    "#### Задание 2.  \n",
    "Для генерации более осмысленного текста сделано:  \n",
    "    1. Алгоритм изменен с GRU на LSTM.  \n",
    "    2. Текст \"alice_in_wonderland.txt\" очищен от лишних символов, пробелов, знаков абзаца и кусков текста, не относящихся к произведению.  \n",
    "    3. Изменены параметры на:  \n",
    "        BATCH_SIZE, HIDDEN_SIZE = 256, 256  \n",
    "        NUM_ITERATIONS = 50 (25 итераций не хватало для того, чтобы получившийся текст был похож на предложения)  \n",
    "Результаты:  \n",
    "Итерация #: 39  \n",
    "but she remembered trying to find that she was now about two feet high, and was just paysing with the glass  \n",
    "Итерация #: 43  \n",
    "stay in here any longer! she waited for a few minutes she heard a viny of the court, and they drew all manner  \n",
    "Итерация #: 45  \n",
    "you know and then she went on, whats your name, child? my name is alice, so please your majesty  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
